{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/'\n",
    "MODEL_PATH = 'models/'\n",
    "SUBMISSION_PATH = 'submissions/'\n",
    "LOG_DIR = 'logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Primitive(Module):\n",
    "    def __init__(self, dim, activation):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(dim, 100)\n",
    "        self.l2 = nn.Linear(100, 1)\n",
    "        self.activation = activation()\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.l2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationModule(Module):\n",
    "    def __init__(self, dim, k, activation):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(dim, 1)\n",
    "        self.V = nn.Parameter(torch.randn(dim, k),requires_grad=True)\n",
    "        self.activation = activation()\n",
    "    def forward(self, x):\n",
    "        h1 = self.W(x)\n",
    "        h21 = torch.matmul(x, self.V).pow(2).sum(1, keepdim=True)\n",
    "        h22 = torch.matmul(x.pow(2), self.V.pow(2)).sum(1, keepdim=True)\n",
    "        h2 = 0.5*(h21 - h22)\n",
    "        h = h1 + h2\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class factorization_set(Dataset):\n",
    "    def __init__(self, X, y = None, maxuid = 943, maxmid = 1682, unlabeled_num = 10000, \n",
    "                 movie_means = None, user_means = None, \n",
    "                 user_dif_rates = None, movie_dif_rates = None):\n",
    "        self.unlabeled_num = unlabeled_num\n",
    "        self.maxuid = maxuid\n",
    "        self.maxmid = maxmid\n",
    "        if not y is None:\n",
    "            self.y = torch.from_numpy(y).view((-1,1)).float()\n",
    "        else:\n",
    "            self.y = y\n",
    "        X_tensor = torch.from_numpy(X - 1)\n",
    "        users = torch.nn.functional.one_hot(X_tensor[:,0], maxuid).float()\n",
    "        movies = torch.nn.functional.one_hot(X_tensor[:,1], maxmid).float()\n",
    "        \n",
    "        self.sample_matrix = np.zeros((maxuid,maxmid), dtype = np.bool)\n",
    "        if user_means is None:\n",
    "            self.user_means = np.zeros(maxuid)\n",
    "            self.movie_means = np.zeros(maxmid)\n",
    "            self.user_dif_rates = np.zeros((maxuid,maxmid))\n",
    "            self.movie_dif_rates = np.zeros((maxmid,maxuid))\n",
    "        else:\n",
    "            self.user_means = user_means\n",
    "            self.movie_means = movie_means\n",
    "            self.user_dif_rates = user_dif_rates\n",
    "            self.movie_dif_rates = movie_dif_rates\n",
    "            \n",
    "        \n",
    "        self.sample_matrix[(X-1)[:,0],(X-1)[:,1]] = True\n",
    "        \n",
    "        self.weights = np.ones(len(X) + unlabeled_num).reshape(-1,1)\n",
    "        alpha = 20.\n",
    "        self.weights = torch.from_numpy(self.weights).view((-1,1)).float()\n",
    "        self.weights[:-unlabeled_num] = alpha\n",
    "        self.weights = torch.sqrt(self.weights)\n",
    "        \n",
    "        uid_mean, uid_inverse = np.unique(X[:,0], return_inverse = True)\n",
    "        uid_mean = uid_mean.reshape(-1,1)\n",
    "        uid_mean = np.concatenate((uid_mean,\n",
    "                    np.zeros((len(uid_mean),1 + maxmid))), axis = 1)\n",
    "        \n",
    "        mid_mean, mid_inverse = np.unique(X[:,1], return_inverse = True)\n",
    "        mid_mean = mid_mean.reshape(-1,1)\n",
    "        mid_mean = np.concatenate((mid_mean,\n",
    "                    np.zeros((len(mid_mean),1 + maxuid))), axis = 1)\n",
    "        #todo prediction modification\n",
    "        if not y is None:\n",
    "            for i in range(len(uid_mean)):\n",
    "                mask = (X[:,0] == uid_mean[i,0])\n",
    "                all_rates = y[mask]\n",
    "                rates_vec = (movies[mask]*all_rates.reshape(-1,1)).sum(axis = 0)\n",
    "                uid_mean[i,1] = all_rates.mean()\n",
    "                self.user_means[(uid_mean[i,0] - 1).astype(np.int)] = uid_mean[i,1]\n",
    "                self.user_dif_rates[(uid_mean[i,0] - 1).astype(np.int)] = rates_vec\n",
    "                uid_mean[i,2:] = rates_vec\n",
    "            for i in range(len(mid_mean)):\n",
    "                mask = (X[:,1] == mid_mean[i,0])\n",
    "                all_rates = y[mask]\n",
    "                rates_vec = (users[mask]*all_rates.reshape(-1,1)).sum(axis = 0)\n",
    "                mid_mean[i,1] = all_rates.mean()\n",
    "                self.movie_means[(mid_mean[i,0] - 1).astype(np.int)] = mid_mean[i,1]\n",
    "                self.movie_dif_rates[(mid_mean[i,0] - 1).astype(np.int)] = rates_vec\n",
    "                mid_mean[i,2:] = rates_vec\n",
    "        else:\n",
    "            for i in range(len(uid_mean)):\n",
    "                uid_mean[i,1] = self.user_means[(uid_mean[i,0] - 1).astype(np.int)]\n",
    "                rates_vec = self.user_dif_rates[(uid_mean[i,0] - 1).astype(np.int)] \n",
    "                uid_mean[i,2:] = rates_vec\n",
    "            for i in range(len(mid_mean)):\n",
    "                mid_mean[i,1] = self.movie_means[(mid_mean[i,0] - 1).astype(np.int)]\n",
    "                rates_vec = self.movie_dif_rates[(mid_mean[i,0] - 1).astype(np.int)]\n",
    "                mid_mean[i,2:] = rates_vec\n",
    "                \n",
    "        #without other rates [,1] and .view((-1,1)) with [,1:]\n",
    "        movie_mean_all = torch.from_numpy(mid_mean[mid_inverse,1]).view(-1,1).float()\n",
    "        user_mean_all = torch.from_numpy(uid_mean[uid_inverse,1]).view(-1,1).float()\n",
    "        \n",
    "        self.X = torch.cat((users, movies,movie_mean_all,\n",
    "                            user_mean_all), 1).float()\n",
    "        \n",
    "        unlabeled_samples = self.get_samples(unlabeled_num)\n",
    "        self.X = torch.cat((self.X, unlabeled_samples), 0)\n",
    "        self.shown_idxs = torch.from_numpy(np.zeros((unlabeled_num,2),dtype = np.int))\n",
    "        self.shown_idxs.copy_(self.idxs)\n",
    "        \n",
    "        if not self.y is None:\n",
    "            unlabeled_targets = torch.zeros(unlabeled_num).view(-1,1)\n",
    "            self.y = torch.cat((self.y, unlabeled_targets),0)\n",
    "            \n",
    "        self.n_dims = self.X.size()[1]\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def get_samples(self, num = 100000):\n",
    "        self.samples = 'empty'\n",
    "        self.idxs = 'empty'\n",
    "        idxs = np.nonzero(~self.sample_matrix)\n",
    "        idxs = np.concatenate((idxs[0].reshape(-1,1), idxs[1].reshape(-1,1)),axis = 1)\n",
    "        np.random.shuffle(idxs)\n",
    "        idxs = idxs[:num]\n",
    "        \n",
    "        movie_mean_all = np.zeros((num,self.maxuid + 1))\n",
    "        movie_mean_all[:,0] = self.movie_means[idxs[:,1]]\n",
    "        movie_mean_all[:,1:] = self.movie_dif_rates[idxs[:,1]]\n",
    "        \n",
    "        user_mean_all = np.zeros((num,self.maxmid + 1))\n",
    "        user_mean_all[:,0] = self.user_means[idxs[:,0]]\n",
    "        user_mean_all[:,1:] = self.user_dif_rates[idxs[:,0]]\n",
    "        self.idxs = torch.from_numpy(idxs)\n",
    "        movie_mean_all = torch.from_numpy(movie_mean_all).float()\n",
    "        user_mean_all = torch.from_numpy(user_mean_all).float()\n",
    "        users = torch.nn.functional.one_hot(self.idxs[:,0], self.maxuid).float()\n",
    "        movies = torch.nn.functional.one_hot(self.idxs[:,1], self.maxmid).float()\n",
    "        \n",
    "        ##important there is feature u can choose [,0] for mean [,0:] for all .view(-1,1)\n",
    "        generated_samples = torch.cat((users, movies,\n",
    "                            movie_mean_all[:,0].view(-1,1),user_mean_all[:,0].view(-1,1)), 1)\n",
    "        \n",
    "        self.samples = generated_samples\n",
    "        return generated_samples\n",
    "        \n",
    "        \n",
    "    def set_samples(self,idxs,pos1, pool = False):\n",
    "        idxs = idxs.view(-1)\n",
    "        if pool:\n",
    "            mark_idxs = self.shown_idxs\n",
    "        else:\n",
    "            mark_idxs = self.idxs\n",
    "        \n",
    "        if pos1 + len(idxs) != 0:\n",
    "            self.X[pos1:pos1 + len(idxs)] = self.samples[idxs]\n",
    "            self.shown_idxs[pos1:pos1 + len(idxs)] = mark_idxs[idxs]\n",
    "        else:\n",
    "            self.X[pos1:] = self.samples[idxs]\n",
    "            self.shown_idxs[pos1:] = mark_idxs[idxs]\n",
    "        \n",
    "        if pool:\n",
    "            if pos1 + len(idxs) != 0:\n",
    "                (self.sample_matrix[mark_idxs[pos1:pos1 + len(idxs),0].numpy(),\n",
    "                                    mark_idxs[pos1:pos1 + len(idxs),1].numpy()]) = True\n",
    "            else:\n",
    "                (self.sample_matrix[mark_idxs[pos1:,0].numpy(),\n",
    "                                    mark_idxs[pos1:,1].numpy()]) = True\n",
    "        else:\n",
    "            self.sample_matrix[mark_idxs[idxs,0].numpy(),mark_idxs[idxs,1].numpy()] = True\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        if not self.y is None:\n",
    "            return self.X[idx], self.y[idx], self.weights[idx]\n",
    "        else:\n",
    "            return self.X[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine:\n",
    "    def __init__(self, dim, activation, loss_fn, k = 100, primitive = False, load = False):\n",
    "        if not load is None:\n",
    "            self.model = torch.load(MODEL_PATH + load)\n",
    "        else:\n",
    "            if primitive:\n",
    "                self.model = Primitive(dim, activation)\n",
    "            else:\n",
    "                self.model = FactorizationModule(dim, k, activation)\n",
    "        self.model.to('cuda')\n",
    "        self.loss_fn = loss_fn()\n",
    "        \n",
    "    def fit(self, X, y, n_epoch = 10, batchsize = 50000, ler = 1e-1, log = True, \n",
    "            update_freq = 10, save_freq = 100, eps=1e-03, start_update = 1000):\n",
    "        self.model.train()\n",
    "        self.batchsize = batchsize\n",
    "        self.print_each = 1\n",
    "        self.dataset = factorization_set(X,y)\n",
    "        self.loader = DataLoader(\n",
    "            self.dataset, batch_size=batchsize, shuffle = True, drop_last=True)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=ler, weight_decay=1e-3)\n",
    "        #self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #    self.optimizer, 'min', verbose = True, patience = 60*len(self.loader))\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                            self.optimizer, 1000, eta_min=1e-5)\n",
    "        torch.optim.lr_scheduler.StepLR(self.optimizer, 1000, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "        self.X_batch = torch.cuda.FloatTensor(batchsize, self.dataset.n_dims)\n",
    "        self.y_batch = torch.cuda.FloatTensor(batchsize, 1)\n",
    "        self.w_batch = torch.cuda.FloatTensor(batchsize, 1)\n",
    "        \n",
    "        losses_per_epoch = [0. for i in range(self.print_each)]\n",
    "        losses_per_batch = [0. for i in range(len(self.loader))]\n",
    "        \n",
    "        min_loss = 1e9\n",
    "        \n",
    "        if log:\n",
    "            self.original_stdout = sys.stdout\n",
    "        \n",
    "        with open(LOG_DIR + 'log.txt', 'w') as f:\n",
    "            if log:\n",
    "                sys.stdout = f\n",
    "            \n",
    "            print('fit begins...')\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            for epoch in range(n_epoch):\n",
    "                for i, batch in enumerate(self.loader):\n",
    "                    X_batch = self.X_batch.copy_(batch[0])\n",
    "                    y_batch = self.y_batch.copy_(batch[1])\n",
    "                    w_batch = self.w_batch.copy_(batch[2])\n",
    "                    h = self.model(X_batch)\n",
    "                    h = w_batch*h\n",
    "                    y_batch = y_batch*w_batch\n",
    "                    loss = self.loss_fn(h,y_batch)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                    losses_per_batch[i] = loss.item()\n",
    "                \n",
    "                losses_per_epoch[epoch%self.print_each] = (sum(losses_per_batch)/len(losses_per_batch))\n",
    "                \n",
    "                if (epoch + 1)%self.print_each == 0:\n",
    "                    print(epoch + 1, sum(losses_per_epoch)/len(losses_per_epoch))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                if (epoch + 1)%save_freq == 0:\n",
    "                    torch.save(self.model, MODEL_PATH + f'{epoch+1}')\n",
    "                \n",
    "                if (epoch + 1 > start_update) and (epoch+1)%update_freq == 0:\n",
    "                    self.model.eval()\n",
    "                    X_w,_, __ = self.dataset[-10000:]\n",
    "                    X_batch = self.X_batch[:10000].copy_(X_w)\n",
    "                    h = self.model(X_batch)\n",
    "                    args = torch.argsort(h, descending=True)\n",
    "                    self.dataset.set_samples(args[:1000], -10000, pool = True)\n",
    "                    for i in range(9,0,-1):\n",
    "                        temp_samples = self.dataset.get_samples(10000)\n",
    "                        X_batch = self.X_batch[:10000].copy_(temp_samples)\n",
    "                        h = self.model(X_batch)\n",
    "                        args = torch.argsort(h, descending=True)\n",
    "                        self.dataset.set_samples(args[:1000], -i*1000)\n",
    "                    self.model.train()\n",
    "                    \n",
    "                    print('updated unlabeled data')\n",
    "            \n",
    "            if log:\n",
    "                sys.stdout = self.original_stdout\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #todo prediction modification\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_set = factorization_set(X,movie_means = self.dataset.movie_means, \n",
    "                                         user_means = self.dataset.user_means, \n",
    "                                         user_dif_rates = self.dataset.user_dif_rates, \n",
    "                                         movie_dif_rates = self.dataset.movie_dif_rates)\n",
    "            answers = []\n",
    "            batch_num = len(test_set)//self.batchsize + 1\n",
    "            for i in range(batch_num):\n",
    "                if i != batch_num - 1:\n",
    "                    X_batch = self.X_batch.copy_(test_set\n",
    "                                [i*self.batchsize:(i+1)*self.batchsize])\n",
    "                else:\n",
    "                    X_batch = (self.X_batch[:len(test_set)%self.batchsize]\n",
    "                               .copy_(test_set[i*self.batchsize:(i+1)*self.batchsize]))\n",
    "                \n",
    "                answer_batch = self.model(X_batch).detach().cpu().numpy().flatten()\n",
    "                answers.append(answer_batch)\n",
    "\n",
    "        answer = np.concatenate(answers)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    with open(fname,'r') as f:\n",
    "        text = f.read()\n",
    "    text = text.split('\\n')\n",
    "    text.pop()\n",
    "    text = [[int(number) for number in line.split()] for line in text]\n",
    "    data = np.array(text, dtype = np.int64)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(DATA_PATH + 'train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "some = np.unique(train_data[:,1])\n",
    "for i in range(some.max()):\n",
    "    if not i+1 in some:\n",
    "        print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "some = np.unique(train_data[:,0])\n",
    "for i in range(some.max()):\n",
    "    if not i+1 in some:\n",
    "        print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 943, 1682], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions = train_data[:,:-1].max(axis = 0)\n",
    "dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2625"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = dimensions.sum()\n",
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = dimension + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2627"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit begins...\n",
      "1 844.3066003084183\n",
      "2 371.677507019043\n",
      "3 210.09121055603026\n",
      "4 122.96328353881836\n",
      "5 74.367631149292\n",
      "6 47.81370086669922\n",
      "7 31.2848762512207\n",
      "8 20.167290115356444\n",
      "9 13.334589004516602\n",
      "10 9.048922491073608\n",
      "11 6.1200361251831055\n",
      "12 4.38610692024231\n",
      "13 3.4207558631896973\n",
      "14 2.902648401260376\n",
      "15 2.580278158187866\n",
      "16 2.329804301261902\n",
      "17 2.096774935722351\n",
      "18 1.9431039094924927\n",
      "19 1.886708927154541\n",
      "20 1.840683937072754\n",
      "21 1.789383554458618\n",
      "22 1.7517578840255736\n",
      "23 1.7299987077713013\n",
      "24 1.7115173101425172\n",
      "25 1.6906277656555175\n",
      "26 1.6777133464813232\n",
      "27 1.6637107133865356\n",
      "28 1.6515342473983765\n",
      "29 1.6387967348098755\n",
      "30 1.6294005155563354\n",
      "31 1.6210793256759644\n",
      "32 1.612468671798706\n",
      "33 1.6049748420715333\n",
      "34 1.5963948965072632\n",
      "35 1.5894833564758302\n",
      "36 1.5828024387359618\n",
      "37 1.5756308078765868\n",
      "38 1.5706130981445312\n",
      "39 1.5644574880599975\n",
      "40 1.5582899093627929\n",
      "41 1.5543992280960084\n",
      "42 1.5477132081985474\n",
      "43 1.5461216688156127\n",
      "44 1.5393113613128662\n",
      "45 1.534599208831787\n",
      "46 1.5329376459121704\n",
      "47 1.5255127191543578\n",
      "48 1.5236429691314697\n",
      "49 1.520575761795044\n",
      "50 1.5156192779541016\n",
      "51 1.512429404258728\n",
      "52 1.5071840047836305\n",
      "53 1.5060791730880738\n",
      "54 1.5008607149124145\n",
      "55 1.4998955249786377\n",
      "56 1.4947759866714478\n",
      "57 1.493393564224243\n",
      "58 1.4907945156097413\n",
      "59 1.4890350580215455\n",
      "60 1.4856292963027955\n",
      "61 1.4833660125732422\n",
      "62 1.479869294166565\n",
      "63 1.4762879848480224\n",
      "64 1.4746976613998413\n",
      "65 1.4715128421783448\n",
      "66 1.4701006650924682\n",
      "67 1.4671732425689696\n",
      "68 1.4669944524765015\n",
      "69 1.466143012046814\n",
      "70 1.4592015266418457\n",
      "71 1.4608384847640992\n",
      "72 1.4567628860473634\n",
      "73 1.4568493604660033\n",
      "74 1.4536939859390259\n",
      "75 1.451359009742737\n",
      "76 1.4511546611785888\n",
      "77 1.448336672782898\n",
      "78 1.446155595779419\n",
      "79 1.4441985607147216\n",
      "80 1.442219614982605\n",
      "81 1.4417125463485718\n",
      "82 1.4389607906341553\n",
      "83 1.4369248390197753\n",
      "84 1.4353073358535766\n",
      "85 1.4339354515075684\n",
      "86 1.4337894439697265\n",
      "87 1.430481243133545\n",
      "88 1.4307742357254027\n",
      "89 1.4288642883300782\n",
      "90 1.4275702238082886\n",
      "91 1.4259710788726807\n",
      "92 1.4252675533294679\n",
      "93 1.423392653465271\n",
      "94 1.4212445259094237\n",
      "95 1.4198678493499757\n",
      "96 1.417838168144226\n",
      "97 1.4161540031433106\n",
      "98 1.4144020795822143\n",
      "99 1.4143667221069336\n",
      "100 1.4113978624343873\n",
      "101 1.410188889503479\n",
      "102 1.4096402883529664\n",
      "103 1.4072619199752807\n",
      "104 1.4073503732681274\n",
      "105 1.4074036359786988\n",
      "106 1.4049444675445557\n",
      "107 1.4041858673095704\n",
      "108 1.4019820928573608\n",
      "109 1.4004900932312012\n",
      "110 1.3999574422836303\n",
      "111 1.3995480298995973\n",
      "112 1.3990062713623046\n",
      "113 1.3965852975845336\n",
      "114 1.3960110902786256\n",
      "115 1.3962425708770752\n",
      "116 1.3949113368988038\n",
      "117 1.3940610885620117\n",
      "118 1.392302441596985\n",
      "119 1.3901309490203857\n",
      "120 1.3905298709869385\n",
      "121 1.389254641532898\n",
      "122 1.3890485763549805\n",
      "123 1.387667751312256\n",
      "124 1.38668794631958\n",
      "125 1.3864450216293336\n",
      "126 1.3861608743667602\n",
      "127 1.3839611291885376\n",
      "128 1.3830796957015992\n",
      "129 1.3828418016433717\n",
      "130 1.3805755615234374\n",
      "131 1.3816489219665526\n",
      "132 1.3793211698532104\n",
      "133 1.3797107696533204\n",
      "134 1.378243637084961\n",
      "135 1.3764800310134888\n",
      "136 1.3773519277572632\n",
      "137 1.377683734893799\n",
      "138 1.3769678115844726\n",
      "139 1.3760661363601685\n",
      "140 1.3742444276809693\n",
      "141 1.374553918838501\n",
      "142 1.374208116531372\n",
      "143 1.3727290153503418\n",
      "144 1.3735445022583008\n",
      "145 1.3703969717025757\n",
      "146 1.371539306640625\n",
      "147 1.3715035676956178\n",
      "148 1.3705466270446778\n",
      "149 1.3702191352844237\n",
      "150 1.3692750930786133\n",
      "151 1.3688884258270264\n",
      "152 1.3693370342254638\n",
      "153 1.3676312685012817\n",
      "154 1.3670127391815186\n",
      "155 1.3657461881637574\n",
      "156 1.3665469646453858\n",
      "157 1.3664658308029174\n",
      "158 1.365880560874939\n",
      "159 1.3660337209701539\n",
      "160 1.3659460067749023\n",
      "161 1.3641227960586548\n",
      "162 1.3650373458862304\n",
      "163 1.3627538204193115\n",
      "164 1.3628103017807007\n",
      "165 1.3628815174102784\n",
      "166 1.3626283407211304\n",
      "167 1.362116813659668\n",
      "168 1.3616283893585206\n",
      "169 1.3619606494903564\n",
      "170 1.3607856512069703\n",
      "171 1.3619382858276368\n",
      "172 1.3618947982788085\n",
      "173 1.3602035760879516\n",
      "174 1.3604284524917603\n",
      "175 1.3594321966171266\n",
      "176 1.3590375661849976\n",
      "177 1.359275221824646\n",
      "178 1.3588843822479248\n",
      "179 1.3590143203735352\n",
      "180 1.3585894346237182\n",
      "181 1.3586930274963378\n",
      "182 1.3581049680709838\n",
      "183 1.3588684320449829\n",
      "184 1.3583396673202515\n",
      "185 1.358749556541443\n",
      "186 1.357977557182312\n",
      "187 1.3582718133926392\n",
      "188 1.3585746526718139\n",
      "189 1.3586841344833374\n",
      "190 1.3583852291107177\n",
      "191 1.357220458984375\n",
      "192 1.3560681104660035\n",
      "193 1.3564802646636962\n",
      "194 1.3568798065185548\n",
      "195 1.3576725006103516\n",
      "196 1.3583454132080077\n",
      "197 1.3574816942214967\n",
      "198 1.3587244510650636\n",
      "199 1.3565051794052123\n",
      "200 1.3576914548873902\n",
      "201 1.4294242858886719\n",
      "202 1.4600806474685668\n",
      "203 1.4819963216781615\n",
      "204 1.5087765455245972\n",
      "205 1.5393882989883423\n",
      "206 1.5988978147506714\n",
      "207 1.6022712230682372\n",
      "208 1.664867115020752\n",
      "209 1.6699162483215333\n",
      "210 1.7108669996261596\n",
      "211 1.6966997623443603\n",
      "212 1.7181066513061523\n",
      "213 1.7314202308654785\n",
      "214 1.7713621854782104\n",
      "215 1.6982887983322144\n",
      "216 1.7485331535339355\n",
      "217 1.6962841272354126\n",
      "218 1.7508976936340332\n",
      "219 1.6882408380508422\n",
      "220 1.6835187435150147\n",
      "221 1.6968496799468995\n",
      "222 1.6831093788146974\n",
      "223 1.7167354822158813\n",
      "224 1.6747581005096435\n",
      "225 1.6835939168930054\n",
      "226 1.6148019552230835\n",
      "227 1.6093098878860475\n",
      "228 1.6049739837646484\n",
      "229 1.5877094745635987\n",
      "230 1.6006015300750733\n",
      "231 1.5940963745117187\n",
      "232 1.578388237953186\n",
      "233 1.56587073802948\n",
      "234 1.5653804302215577\n",
      "235 1.5686834335327149\n",
      "236 1.5731975078582763\n",
      "237 1.5675958156585694\n",
      "238 1.5464781522750854\n",
      "239 1.5546120643615722\n",
      "240 1.553724431991577\n",
      "241 1.534417200088501\n",
      "242 1.52845356464386\n",
      "243 1.524260663986206\n",
      "244 1.523310375213623\n",
      "245 1.5220128297805786\n",
      "246 1.5126084327697753\n",
      "247 1.5065005779266358\n",
      "248 1.5127971410751342\n",
      "249 1.5037410497665404\n",
      "250 1.5070191383361817\n",
      "251 1.499847674369812\n",
      "252 1.4880409240722656\n",
      "253 1.4942378044128417\n",
      "254 1.4733421325683593\n",
      "255 1.470129656791687\n",
      "256 1.462288737297058\n",
      "257 1.4627470254898072\n",
      "258 1.4547378301620484\n",
      "259 1.454244351387024\n",
      "260 1.446496272087097\n",
      "261 1.4482020139694214\n",
      "262 1.4474129676818848\n",
      "263 1.4445617914199829\n",
      "264 1.4502381086349487\n",
      "265 1.4380732774734497\n",
      "266 1.4302977323532104\n",
      "267 1.434911274909973\n",
      "268 1.4299267053604126\n",
      "269 1.425653910636902\n",
      "270 1.427197480201721\n",
      "271 1.41678626537323\n",
      "272 1.4163321495056151\n",
      "273 1.4172099113464356\n",
      "274 1.4150853633880616\n",
      "275 1.412255859375\n",
      "276 1.4125978469848632\n",
      "277 1.407477331161499\n",
      "278 1.4010936737060546\n",
      "279 1.4018624067306518\n",
      "280 1.3944915533065796\n",
      "281 1.3991877079010009\n",
      "282 1.3916738510131836\n",
      "283 1.3937594890594482\n",
      "284 1.3848379373550415\n",
      "285 1.3873405933380127\n",
      "286 1.3769428730010986\n",
      "287 1.37602858543396\n",
      "288 1.3731816291809082\n",
      "289 1.3671454906463623\n",
      "290 1.3683407306671143\n",
      "291 1.3669116258621217\n",
      "292 1.3640091180801392\n",
      "293 1.3615212440490723\n",
      "294 1.362588930130005\n",
      "295 1.3614601373672486\n",
      "296 1.3581351518630982\n",
      "297 1.3591521501541137\n",
      "298 1.3565498828887939\n",
      "299 1.3539599657058716\n",
      "300 1.3481898069381715\n",
      "301 1.347425103187561\n",
      "302 1.3447048902511596\n",
      "303 1.3421032667160033\n",
      "304 1.3400039196014404\n",
      "305 1.338223385810852\n",
      "306 1.3382076501846314\n",
      "307 1.3344700813293457\n",
      "308 1.3335100650787353\n",
      "309 1.3337623596191406\n",
      "310 1.3333616495132445\n",
      "311 1.3298818111419677\n",
      "312 1.3309407472610473\n",
      "313 1.3254051208496094\n",
      "314 1.32567617893219\n",
      "315 1.3246814727783203\n",
      "316 1.3222782135009765\n",
      "317 1.3223667860031127\n",
      "318 1.3191024303436278\n",
      "319 1.315802264213562\n",
      "320 1.3157742500305176\n",
      "321 1.3143241643905639\n",
      "322 1.3123308181762696\n",
      "323 1.3123497009277343\n",
      "324 1.312839412689209\n",
      "325 1.311140775680542\n",
      "326 1.3077364683151245\n",
      "327 1.306927537918091\n",
      "328 1.3067382335662843\n",
      "329 1.3048210859298706\n",
      "330 1.3033310651779175\n",
      "331 1.302248477935791\n",
      "332 1.301577663421631\n",
      "333 1.2974352836608887\n",
      "334 1.2988593339920045\n",
      "335 1.2965470552444458\n",
      "336 1.297417402267456\n",
      "337 1.2969212293624879\n",
      "338 1.2961183786392212\n",
      "339 1.2947059631347657\n",
      "340 1.2935043811798095\n",
      "341 1.2921550273895264\n",
      "342 1.293494701385498\n",
      "343 1.2906134366989135\n",
      "344 1.2898946523666381\n",
      "345 1.288215970993042\n",
      "346 1.2883653402328492\n",
      "347 1.2877463340759276\n",
      "348 1.287112331390381\n",
      "349 1.286824893951416\n",
      "350 1.285884428024292\n",
      "351 1.2851623058319093\n",
      "352 1.2830090045928955\n",
      "353 1.2837496995925903\n",
      "354 1.2834877014160155\n",
      "355 1.2826878786087037\n",
      "356 1.2828691005706787\n",
      "357 1.2819427251815796\n",
      "358 1.2810548543930054\n",
      "359 1.2814247846603393\n",
      "360 1.279795527458191\n",
      "361 1.2802966594696046\n",
      "362 1.2793020963668824\n",
      "363 1.278896737098694\n",
      "364 1.2788182973861695\n",
      "365 1.2766919136047363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366 1.2770514488220215\n",
      "367 1.2769563436508178\n",
      "368 1.277229404449463\n",
      "369 1.2774723529815675\n",
      "370 1.274705672264099\n",
      "371 1.2744000196456908\n",
      "372 1.2746735095977784\n",
      "373 1.2741244554519653\n",
      "374 1.2745604991912842\n",
      "375 1.2735772848129272\n",
      "376 1.2745623350143434\n",
      "377 1.2742133855819702\n",
      "378 1.2733174562454224\n",
      "379 1.2717703104019165\n",
      "380 1.2731369972229003\n",
      "381 1.2728600978851319\n",
      "382 1.2728051900863648\n",
      "383 1.273056697845459\n",
      "384 1.272372341156006\n",
      "385 1.2720932245254517\n",
      "386 1.2720498085021972\n",
      "387 1.2715240240097045\n",
      "388 1.2712956666946411\n",
      "389 1.2710339784622193\n",
      "390 1.2712174654006958\n",
      "391 1.2706684589385986\n",
      "392 1.2708056449890137\n",
      "393 1.270956325531006\n",
      "394 1.2702241897583009\n",
      "395 1.271064782142639\n",
      "396 1.2700422286987305\n",
      "397 1.2712478160858154\n",
      "398 1.270595908164978\n",
      "399 1.2701725721359254\n",
      "400 1.2707071781158448\n",
      "401 1.39321870803833\n",
      "402 34.242176222801206\n",
      "403 688.9356872558594\n",
      "404 294.287557220459\n",
      "405 211.40211181640626\n",
      "406 125.5978775024414\n",
      "407 82.7266944885254\n",
      "408 50.62587776184082\n",
      "409 37.795912170410155\n",
      "410 21.558060073852538\n",
      "411 17.380228996276855\n",
      "412 11.699103546142577\n",
      "413 8.382377624511719\n",
      "414 6.96613712310791\n",
      "415 5.160089302062988\n",
      "416 4.272982406616211\n",
      "417 3.6676142692565916\n",
      "418 3.1275655746459963\n",
      "419 2.8067493438720703\n",
      "420 2.5732150077819824\n",
      "421 2.3506168842315676\n",
      "422 2.2109593391418456\n",
      "423 2.088476324081421\n",
      "424 1.9826685428619384\n",
      "425 1.9077725887298584\n",
      "426 1.8521990060806275\n",
      "427 1.7936330556869506\n",
      "428 1.74880211353302\n",
      "429 1.7124286413192749\n",
      "430 1.6815035820007325\n",
      "431 1.6539051532745361\n",
      "432 1.6294554710388183\n",
      "433 1.6086501598358154\n",
      "434 1.5906838178634644\n",
      "435 1.5722911357879639\n",
      "436 1.5545596122741698\n",
      "437 1.542816972732544\n",
      "438 1.5308526515960694\n",
      "439 1.5174612998962402\n",
      "440 1.507502007484436\n",
      "441 1.5010764122009277\n",
      "442 1.4919704914093017\n",
      "443 1.4842575550079347\n",
      "444 1.4749183654785156\n",
      "445 1.4670048236846924\n",
      "446 1.459239625930786\n",
      "447 1.4509235143661499\n",
      "448 1.4477453231811523\n",
      "449 1.4422322034835815\n",
      "450 1.4390634059906007\n",
      "451 1.4329524040222168\n",
      "452 1.427906084060669\n",
      "453 1.4235102891921998\n",
      "454 1.4206420183181763\n",
      "455 1.4179086446762086\n",
      "456 1.4128511428833008\n",
      "457 1.4089410066604615\n",
      "458 1.4015496015548705\n",
      "459 1.4006224393844604\n",
      "460 1.3985464096069335\n",
      "461 1.3985469579696654\n",
      "462 1.3977355480194091\n",
      "463 1.3901731967926025\n",
      "464 1.388822889328003\n",
      "465 1.3865124702453613\n",
      "466 1.381100630760193\n",
      "467 1.3792468309402466\n",
      "468 1.376599621772766\n",
      "469 1.3756986379623413\n",
      "470 1.3732025384902955\n",
      "471 1.3683345556259154\n",
      "472 1.3724894523620605\n",
      "473 1.364859127998352\n",
      "474 1.3645867109298706\n",
      "475 1.361375617980957\n",
      "476 1.3586401462554931\n",
      "477 1.3579622507095337\n",
      "478 1.3565237045288085\n",
      "479 1.3549084901809691\n",
      "480 1.3494381427764892\n",
      "481 1.3495989084243774\n",
      "482 1.348764681816101\n",
      "483 1.3468765258789062\n",
      "484 1.3456560850143433\n",
      "485 1.3429927349090576\n",
      "486 1.3417422294616699\n",
      "487 1.3405256748199463\n",
      "488 1.3381733655929566\n",
      "489 1.3372762203216553\n",
      "490 1.3343597173690795\n",
      "491 1.3336117029190064\n",
      "492 1.332187867164612\n",
      "493 1.3311250686645508\n",
      "494 1.3287647008895873\n",
      "495 1.3301422834396361\n",
      "496 1.3291660308837892\n",
      "497 1.3276691436767578\n",
      "498 1.3274619817733764\n",
      "499 1.3227267503738402\n",
      "500 1.3222577810287475\n"
     ]
    }
   ],
   "source": [
    "machine = FactorizationMachine(dimension, torch.nn.ReLU, torch.nn.MSELoss, k = 100, primitive = False, load = '5000')\n",
    "machine.fit(train_data[:,:-1],train_data[:,-1], n_epoch = 500, batchsize = 20000, \n",
    "            ler = 1e-2, log = False, update_freq = 100, save_freq = 100, start_update = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer(answer, fname):\n",
    "    with open(SUBMISSION_PATH + fname,'w') as fout:\n",
    "        fout.write('Id,Score\\n')\n",
    "        for i in range(len(answer)):\n",
    "            fout.write(f'{i+1},{answer[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_data(DATA_PATH + 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = machine.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(answer, 'Primitive_submission.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine.dataset[-10001]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
